{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee529a3-f5c9-4a62-b27a-0aee2da8c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbab3c3-0ab1-4f1e-a210-d9f6361c5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c6c9c-689c-48fd-bd16-de2d7f133179",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396439ee-f9e8-4f9a-a1ec-8213ce4cee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install networkx==3.1\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d905e76-4255-47c9-8568-51796e4a32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265025ae-8bfd-46f9-be15-5f50d1a6b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-small-en-v1.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cbee6-4e8f-4210-a7d2-3d9ce63660a5",
   "metadata": {},
   "source": [
    "We'll need to load our player data from our scraped JSON file that stores MLB player data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "696cf153-b8a9-4c6e-81b3-e25d34421139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = 'cs242-project/myfirstproject/myfirstproject/mlb_data_adjusted_bios.json'\n",
    "\n",
    "# load the JSON data from file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# filter to include only dictionaries with non-empty 'biographical_information'\n",
    "valid_entries = [entry for entry in data if 'biographical_information' in entry and entry['biographical_information'].strip()]\n",
    "\n",
    "# create the sentences list from the filtered valid_entries list\n",
    "sentences = [entry['biographical_information'] for entry in valid_entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506b3da-9cea-4cb7-8efa-12f628ef5971",
   "metadata": {},
   "source": [
    "Now we need to tokenize the above sentences and prepare them for input to the BERT model. We'll create a dictionary with two keys: `input_ids` and `attention_mask`. These will be populated with the tokenized versions of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6abd30a4-74ba-4382-9527-bbddb431605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dictionary to store tokenized sentences\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in sentences:\n",
    "    # encode each sentence and append to dictionary\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=512,\n",
    "                                       truncation=True, padding='max_length',\n",
    "                                       return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771b3c2-c715-4e1d-8e16-c098eabfb81a",
   "metadata": {},
   "source": [
    "The next section will prepare the tokenized sentences for processing with the BERT model by reformatting the lists of tensors into single tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "17d368b0-d17c-444c-8a51-09a1bdc1ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat list of tensors into single tensor\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b096f3-02b9-43bf-bee3-b7b2e98ee17b",
   "metadata": {},
   "source": [
    "Below, the BERT model will process the tokenized inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dad79398-f976-414c-bdb7-de73d438bac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6ced3-c2a2-4003-b274-eb79caa01beb",
   "metadata": {},
   "source": [
    "This will extract the embeddings from the `last_hidden_state` tensor from the model's outputs to the `embeddings` variable. `last_hidden_state` is a tensor containing the final layer's hidden states for each token in each input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "792515c2-d7b2-4fb1-9f8a-9916c39b77e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 512, 384])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "54247cef-7126-4ad9-977f-3c10ef63c797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9333e-01,  2.9218e-01, -3.1649e-02,  ..., -6.4065e-01,\n",
       "           3.5040e-01,  1.7890e-01],\n",
       "         [-2.8097e-01,  1.7903e-01,  3.7085e-01,  ..., -6.2551e-01,\n",
       "           7.4291e-01,  6.9920e-01],\n",
       "         [-3.7459e-01,  2.1804e-01,  3.3417e-01,  ..., -2.5003e-01,\n",
       "           5.9015e-02, -4.2579e-01],\n",
       "         ...,\n",
       "         [-2.9276e-01,  3.5609e-02,  1.1313e-01,  ..., -4.1203e-01,\n",
       "           4.9604e-01,  3.3489e-01],\n",
       "         [-4.9367e-02,  1.4166e-01,  2.6685e-01,  ..., -5.0167e-01,\n",
       "           6.2489e-01,  3.1618e-01],\n",
       "         [-1.9331e-01,  2.9219e-01, -3.1652e-02,  ..., -6.4064e-01,\n",
       "           3.5043e-01,  1.7888e-01]],\n",
       "\n",
       "        [[-3.5371e-01, -1.2444e-02,  1.5510e-01,  ..., -4.7077e-01,\n",
       "           2.7036e-01,  8.0258e-01],\n",
       "         [-1.0474e-01, -1.9709e-02,  3.0545e-01,  ..., -5.2183e-02,\n",
       "           6.7888e-01,  8.8091e-01],\n",
       "         [ 3.6925e-02, -5.8432e-02,  4.1652e-01,  ..., -2.8672e-01,\n",
       "           1.3656e+00, -1.9288e-01],\n",
       "         ...,\n",
       "         [-3.5351e-01, -1.1991e-02,  1.5534e-01,  ..., -4.7101e-01,\n",
       "           2.7031e-01,  8.0277e-01],\n",
       "         [-3.5356e-01, -1.2137e-02,  1.5527e-01,  ..., -4.7091e-01,\n",
       "           2.7030e-01,  8.0283e-01],\n",
       "         [-1.1912e-01, -1.7042e-01,  4.4128e-01,  ..., -5.9849e-01,\n",
       "           3.0259e-01,  4.5405e-01]],\n",
       "\n",
       "        [[-3.6840e-01,  2.9674e-01, -3.4416e-01,  ..., -6.2969e-01,\n",
       "           5.6454e-02,  5.5785e-01],\n",
       "         [-2.6483e-01,  2.4417e-01,  2.3666e-02,  ..., -9.6004e-01,\n",
       "           3.3388e-01,  5.7099e-01],\n",
       "         [-3.7640e-02,  2.4563e-01, -7.0749e-02,  ...,  1.9687e-01,\n",
       "          -4.4934e-01,  1.9123e-01],\n",
       "         ...,\n",
       "         [-3.2929e-01,  2.0826e-01, -2.3304e-01,  ..., -2.9280e-01,\n",
       "           1.3955e-01,  4.5067e-01],\n",
       "         [-2.5854e-01,  4.8096e-02, -2.3754e-01,  ..., -4.3096e-01,\n",
       "          -5.5500e-02,  3.9903e-01],\n",
       "         [-2.0978e-01,  2.1032e-02, -1.3060e-01,  ..., -6.0470e-01,\n",
       "          -1.7105e-01,  2.7322e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.0985e-01,  2.2850e-02,  2.4506e-01,  ..., -4.8757e-01,\n",
       "           1.1581e-01,  2.1534e-01],\n",
       "         [-5.8658e-01,  4.3727e-01,  5.7937e-01,  ..., -2.7204e-01,\n",
       "           4.4449e-01,  6.0829e-01],\n",
       "         [-7.3234e-01, -3.3035e-01,  2.4137e-01,  ..., -2.0508e-01,\n",
       "           3.0640e-01, -4.0027e-01],\n",
       "         ...,\n",
       "         [-1.1183e-01,  1.1100e-01,  4.2674e-01,  ..., -9.2368e-01,\n",
       "           6.3244e-01, -4.0694e-01],\n",
       "         [-1.1444e-01, -1.4086e-01,  4.5824e-01,  ..., -6.3877e-01,\n",
       "           2.6400e-01, -1.0040e-02],\n",
       "         [-1.1343e-01, -4.6903e-02,  4.6178e-01,  ..., -4.6019e-01,\n",
       "           7.5247e-01, -1.9543e-02]],\n",
       "\n",
       "        [[-2.8079e-01, -1.4329e-01, -6.8471e-01,  ...,  2.0859e-01,\n",
       "           6.3383e-01,  2.8284e-01],\n",
       "         [-3.3343e-01,  8.5617e-03, -3.0342e-01,  ...,  3.7735e-01,\n",
       "           7.8860e-01,  6.5541e-01],\n",
       "         [-6.6310e-01, -1.7582e-01, -4.6388e-01,  ...,  1.7215e-03,\n",
       "           9.4299e-01,  3.6630e-01],\n",
       "         ...,\n",
       "         [-1.1431e-01, -5.1506e-01, -6.3951e-01,  ...,  1.3137e-01,\n",
       "           7.4668e-01,  1.8799e-01],\n",
       "         [-2.7895e-01, -1.3922e-01, -6.8186e-01,  ...,  2.1027e-01,\n",
       "           6.3285e-01,  2.8650e-01],\n",
       "         [-7.0629e-02, -5.8552e-01, -7.0034e-01,  ...,  1.7453e-01,\n",
       "           7.0993e-01,  2.4056e-01]],\n",
       "\n",
       "        [[-1.7372e-01, -5.2203e-03, -4.4368e-01,  ...,  8.1359e-02,\n",
       "           3.9973e-01,  8.8960e-01],\n",
       "         [-2.5173e-01,  3.3138e-01,  2.5140e-01,  ...,  3.2572e-01,\n",
       "           1.6459e-01,  7.0680e-01],\n",
       "         [-4.3333e-01,  3.1760e-01, -5.4677e-02,  ...,  2.6609e-01,\n",
       "           5.1004e-01,  7.0337e-01],\n",
       "         ...,\n",
       "         [-2.3371e-01,  3.0734e-03,  4.1966e-04,  ...,  8.2774e-02,\n",
       "           6.8287e-01,  1.3764e+00],\n",
       "         [-9.8206e-02,  2.3751e-01, -7.7801e-02,  ..., -1.7473e-01,\n",
       "           4.8785e-01,  2.8939e-01],\n",
       "         [-1.7371e-01, -5.2094e-03, -4.4368e-01,  ...,  8.1383e-02,\n",
       "           3.9974e-01,  8.8956e-01]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43705851-8ced-4c91-ad61-398ee603800d",
   "metadata": {},
   "source": [
    "After we have produced our dense vectors embeddings, we need to perform a mean pooling operation to create a single vector encoding (the sentence embedding). To do this mean pooling operation, we will need to multiply each value in our embeddings tensor by its respective attention_mask value — so that we ignore non-real tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e62c4382-fc9e-47b7-ace9-90876fc565f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 512])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize our attention_mask tensor:\n",
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b491e3d8-4d22-4e08-a2bf-8d00e3f472d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 512, 384])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c94166-8bb7-44b3-8f14-39b9f832b131",
   "metadata": {},
   "source": [
    "Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's attention_mask status. Then we multiply the two tensors to apply the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9f214908-d7bf-48aa-a434-800858a5d46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 512, 384])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4c604-715f-420f-9fdb-2ac9459a2023",
   "metadata": {},
   "source": [
    "\"Mean Pooling\" starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6802dde8-032f-4525-bd7b-6d6d1bbc2536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 384])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we sum the remained of the embeddings along axis 1, because we want to reduce the 512 tokens to 1 dimension\n",
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9057ad-35f2-48ea-8f65-ab825161377e",
   "metadata": {},
   "source": [
    "we want to count only those values that we want to give attention\n",
    "then divide by the sum to get the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2875ec78-3d6a-4334-ac3a-de18ab6bb23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 384])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clamp returns the same tensor with a range given, clamp is used to replace the zeros to a very minimal value\n",
    "# to avoid divide by zero error\n",
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f27de-eee2-4ced-95dd-8d1c84d73bf6",
   "metadata": {},
   "source": [
    "Finally, we calculate the mean as the sum of the embedding activations summed divided by the number of values that should be given attention in each position `summed_mask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d68564e3-c7e9-4008-84a6-e07eeeb451f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled = summed / summed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88430bd-b04a-41d9-a8c6-040389ac4d00",
   "metadata": {},
   "source": [
    "`mean_pooled` is the final \"dense representation\" of the sentences, note that mean_pooled contains all representations for all sentences together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "626ff959-9c31-4568-aa62-53a971153065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1845,  0.3163,  0.2080,  ..., -0.6100,  0.4914,  0.1752],\n",
       "        [-0.2176,  0.0125,  0.3889,  ..., -0.4580,  0.4477,  0.6827],\n",
       "        [-0.3200,  0.2935, -0.0938,  ..., -0.5611,  0.1503,  0.4513],\n",
       "        ...,\n",
       "        [-0.4644, -0.0700,  0.4939,  ..., -0.4268,  0.3867,  0.2270],\n",
       "        [-0.1985, -0.1751, -0.6110,  ...,  0.2191,  0.7615,  0.3476],\n",
       "        [-0.1451,  0.0103, -0.0597,  ...,  0.1133,  0.3767,  0.8671]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8d3e2-cb93-4966-85c8-52c0e3443a96",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d8e292d6-21f2-4250-881a-4bb9652ce67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embedding(query):\n",
    "    tokens = {'input_ids': [], 'attention_mask': []}\n",
    "    new_tokens = tokenizer.encode_plus(query, max_length=512,\n",
    "                                       truncation=True, padding='max_length',\n",
    "                                       return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "    tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "    tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    mean_pooled = summed / summed_mask\n",
    "    \n",
    "    return mean_pooled[0] # assuming query is a single sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a1eb20a7-b6aa-419a-9126-adaffa54a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6eb96482-d2c3-4d3b-8732-767683bf8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Yelich\"\n",
    "query_embedding = convert_to_embedding(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e0f95da6-71d0-47dd-886a-b9ed601e5c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 384])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "41a5033c-e6cf-46a8-9653-0d29b0562795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5504, 0.4686, 0.4785, 0.4296, 0.5287, 0.5336, 0.6901, 0.6901, 0.4773,\n",
       "        0.5255, 0.5255, 0.5352, 0.4463, 0.5299])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity()\n",
    "sim = cos(query_embedding, mean_pooled)\n",
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850408bb-8d7c-4f96-9388-d119407938f1",
   "metadata": {},
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bb7c08e3-270f-443c-81b7-85c67ee2dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "import faiss                   # make faiss available\n",
    "index = faiss.IndexFlatIP(384) # build the index\n",
    "print(index.is_trained)\n",
    "index.add(mean_pooled)         # add vectors to the index\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3be0a8c4-61b4-4801-92d7-63cbf6657675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 384])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d793c9cb-27eb-4505-9e1c-41d85592bd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ed57aaed-351a-48d4-bef5-62a39a61834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(query_embedding[None, :], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6da8a1b8-c0ab-4568-b100-1150addcf9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46.861103]], dtype=float32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f6d5efcf-c042-4d43-af51-027d9c158fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6a94e449-cbd6-4bc8-898c-8b07da14dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index,\"sample_code.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "28123327-d986-493d-a413-b6ed621f4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_loaded = faiss.read_index(\"sample_code.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f2fe74a9-69c6-47db-a271-1da7d0989615",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index_loaded.search(query_embedding[None, :], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "32287a19-0e0a-4e72-8042-8f1bf431c989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46.861103, 46.861103, 37.467735, 36.620792]], dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9b7b7b2f-ab4a-4f26-84a6-417c51951300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 6, 0, 4]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca72a5-ff4e-4a68-8e7d-b829051bbabd",
   "metadata": {},
   "source": [
    "Let's print out the results of the highest scoring document as a result of our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "34709399-0973-4a45-baa2-7cee5d8dfefb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Most similar document based on the query: Christian Yelich made his major league debut in 2013 . He hit .455 as a high school senior , with 14 doubles and 9 home runs in 77 at-bats ; he also stole 27 bases . Baseball America named him a second-team high school All-American infielder , behind first-teamers Sean Coyle , Nick Delmonico , Manny Machado and Chris Hawkins . He was the first first baseman taken in the 2010 amateur draft . He went 23rd overall , to the Florida Marlins ; the scout was Tim McDonnell . He was converted to the outfield after signing with the Marlins ( for a $ 1.7 million bonus ) and began his pro career in 2010 . He showed he belonged right away , going 3 for 3 with a triple , two runs and two RBI in his pro debut . He saw limited action for the GCL Marlins ( 9 for 24 , 2B , 3B , 2 BB , SB ) and Greensboro Grasshoppers ( 8 for 23 , 2 2B , BB ) that summer . A teenager in a full-season league in 2011 , he more than held his own for Greensboro , hitting .312/.388/.484 with 32 doubles , 15 home runs , 73 runs , 77 RBI and 32 steals in 37 tries . He finished among the South Atlantic League leaders in average ( 3rd behind Brady Shoemaker and Andrew Clark ) , OBP ( 7th ) , OPS ( 10th ) , hits ( 144 , tied for 3rd with Noah Perio ) , doubles ( tied for 9th ) and steals ( tied for 6th ) . In the league finale , he helped Greensboro to a title . In game one , he hit a walk-off , two-run , bottom-of-the-15th homer off Jose Monegro . He made the SAL All-Star outfield alongside Brandon Jacobs and Kyle Parker . Baseball America named him the SAL 's # 5 prospect ( behind Bryce Harper , Machado , Jurickson Profar and Jameson Taillon ) , as the Marlins ' top prospect and as the # 41 prospect in all of the minors . He won the Marlins Minor League Player of the Year award . With the Jupiter Hammerheads for most of 2012 ( he had one game for the GCL Marlins ) , he hit .330/.404/.519 with 29 doubles , 76 runs and 20 steals while being caught 6 times . He was all over the Florida State League leaderboards : 2nd in average ( .012 behind Mike O'Neill ) , first in slugging ( by 36 points ) , 2nd in OBP ( 38 points behind O'Neill ) , first in OPS ( .063 ahead of O'Neill ) , tied for 5th in doubles , 3rd in runs ( behind Marcell Ozuna and Ramon Flores ) and 7th in total bases ( 206 ) . He joined O'Neill and Ozuna as the All-Star outfielders . He repeated as Miami 's Minor League Player of the Year .\n"
     ]
    }
   ],
   "source": [
    "most_similar_document_index = I[0][0]  # access the first (and only) element of I since we searched for top 1 document\n",
    "print(I[0][0])\n",
    "most_similar_document = valid_entries[most_similar_document_index][\"biographical_information\"]\n",
    "print(\"Most similar document based on the query:\", most_similar_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b29c88-35f4-476e-b6ea-6ffb4d1597e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
